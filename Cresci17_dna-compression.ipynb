{"cells":[{"cell_type":"markdown","metadata":{"id":"l-2mtRPa6n37"},"source":["### Bot Detection using Digital DNA Compression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20174,"status":"ok","timestamp":1644237701214,"user":{"displayName":"Mahesh chandra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpSxS5Pf7uvPBbQsZ2Ztm996DUzNgWredGCjaOCQ=s64","userId":"16718463301519824511"},"user_tz":-330},"id":"tjN-EQ-l6pY4","outputId":"d291fc11-38ae-4778-d0ee-8905b1cf667e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ryA_Jbv7jWu","outputId":"2487ccf4-cbed-4e68-f68f-31ab1699e8b3","executionInfo":{"status":"ok","timestamp":1644237737096,"user_tz":-330,"elapsed":35894,"user":{"displayName":"Mahesh chandra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpSxS5Pf7uvPBbQsZ2Ztm996DUzNgWredGCjaOCQ=s64","userId":"16718463301519824511"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  data.zip\n","   creating: datasets_full.csv/\n","  inflating: datasets_full.csv/crowdflower_results.csv.zip  \n","  inflating: datasets_full.csv/fake_followers.csv.zip  \n","  inflating: datasets_full.csv/genuine_accounts.csv.zip  \n","  inflating: datasets_full.csv/social_spambots_1.csv.zip  \n","  inflating: datasets_full.csv/social_spambots_2.csv.zip  \n","  inflating: datasets_full.csv/social_spambots_3.csv.zip  \n","  inflating: datasets_full.csv/traditional_spambots_1.csv.zip  \n","  inflating: datasets_full.csv/traditional_spambots_2.csv.zip  \n","  inflating: datasets_full.csv/traditional_spambots_3.csv.zip  \n","  inflating: datasets_full.csv/traditional_spambots_4.csv.zip  \n","  inflating: READ.ME                 \n","  inflating: datasets_full.csv/READ.ME  \n","Archive:  /content/datasets_full.csv/crowdflower_results.csv.zip\n","   creating: crowdflower_results.csv/\n","  inflating: crowdflower_results.csv/crowdflower_results_aggregated.csv  \n","   creating: __MACOSX/\n","   creating: __MACOSX/crowdflower_results.csv/\n","  inflating: __MACOSX/crowdflower_results.csv/._crowdflower_results_aggregated.csv  \n","  inflating: crowdflower_results.csv/crowdflower_results_contributors.csv  \n","  inflating: __MACOSX/crowdflower_results.csv/._crowdflower_results_contributors.csv  \n","  inflating: crowdflower_results.csv/crowdflower_results_detailed.csv  \n","  inflating: __MACOSX/crowdflower_results.csv/._crowdflower_results_detailed.csv  \n","Archive:  /content/datasets_full.csv/fake_followers.csv.zip\n","   creating: fake_followers.csv/\n","  inflating: fake_followers.csv/tweets.csv  \n","   creating: __MACOSX/fake_followers.csv/\n","  inflating: __MACOSX/fake_followers.csv/._tweets.csv  \n","  inflating: fake_followers.csv/users.csv  \n","  inflating: __MACOSX/fake_followers.csv/._users.csv  \n","  inflating: __MACOSX/._fake_followers.csv  \n","Archive:  /content/datasets_full.csv/genuine_accounts.csv.zip\n","   creating: genuine_accounts.csv/\n","  inflating: genuine_accounts.csv/tweets.csv  \n","  inflating: genuine_accounts.csv/users.csv  \n","Archive:  /content/datasets_full.csv/social_spambots_1.csv.zip\n","   creating: social_spambots_1.csv/\n","  inflating: social_spambots_1.csv/tweets.csv  \n","  inflating: social_spambots_1.csv/users.csv  \n","Archive:  /content/datasets_full.csv/social_spambots_2.csv.zip\n","   creating: social_spambots_2.csv/\n","  inflating: social_spambots_2.csv/tweets.csv  \n","  inflating: social_spambots_2.csv/users.csv  \n","Archive:  /content/datasets_full.csv/social_spambots_3.csv.zip\n","   creating: social_spambots_3.csv/\n","  inflating: social_spambots_3.csv/tweets.csv  \n","  inflating: social_spambots_3.csv/users.csv  \n","unzip:  cannot find or open /content/datasets_full.csv/social_spambots_4.csv.zip, /content/datasets_full.csv/social_spambots_4.csv.zip.zip or /content/datasets_full.csv/social_spambots_4.csv.zip.ZIP.\n","Archive:  /content/datasets_full.csv/traditional_spambots_1.csv.zip\n","   creating: traditional_spambots_1.csv/\n","  inflating: traditional_spambots_1.csv/tweets.csv  \n","  inflating: traditional_spambots_1.csv/users.csv  \n","Archive:  /content/datasets_full.csv/traditional_spambots_2.csv.zip\n","   creating: traditional_spambots_2.csv/\n","  inflating: traditional_spambots_2.csv/users.csv  \n","Archive:  /content/datasets_full.csv/traditional_spambots_3.csv.zip\n","   creating: traditional_spambots_3.csv/\n","  inflating: traditional_spambots_3.csv/users.csv  \n","Archive:  /content/datasets_full.csv/traditional_spambots_4.csv.zip\n","   creating: traditional_spambots_4.csv/\n","  inflating: traditional_spambots_4.csv/users.csv  \n"]}],"source":["!cp \"/content/drive/MyDrive/Cresci17/cresci-2017.csv.zip\" \"data.zip\"\n","\n","!unzip data.zip\n","!unzip /content/datasets_full.csv/crowdflower_results.csv.zip\n","!unzip /content/datasets_full.csv/fake_followers.csv.zip\n","!unzip /content/datasets_full.csv/genuine_accounts.csv.zip\n","!unzip /content/datasets_full.csv/social_spambots_1.csv.zip\n","!unzip /content/datasets_full.csv/social_spambots_2.csv.zip\n","!unzip /content/datasets_full.csv/social_spambots_3.csv.zip\n","\n","!unzip /content/datasets_full.csv/traditional_spambots_1.csv.zip\n","!unzip /content/datasets_full.csv/traditional_spambots_2.csv.zip\n","!unzip /content/datasets_full.csv/traditional_spambots_3.csv.zip\n","!unzip /content/datasets_full.csv/traditional_spambots_4.csv.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C79l2PxX6n4C"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BX1qjSxD6n4E"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7-qYYhL6n4F"},"outputs":[],"source":["import random\n","import sys\n","import time\n","import zlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5KW8yA06n4G"},"outputs":[],"source":["from sklearn import utils\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n","from scipy.stats import ttest_1samp"]},{"cell_type":"markdown","metadata":{"id":"NJ0Lp0qg6n4H"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"PwJNaeeF6n4H"},"source":["### Digital DNA\n","\n","Model account behaviour by defining the following alphabet, of cardinality N = 3,\n","\n","$B^3_{type} = {\\{A, C, T\\}}$\n","\n","A $\\leftarrow$ tweet,\n","\n","C $\\leftarrow$ reply,\n","\n","T $\\leftarrow$ retweet"]},{"cell_type":"markdown","metadata":{"id":"H62zMUib6n4J"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"Lp9CtdH-6n4K"},"source":["#### Datasets"]},{"cell_type":"markdown","metadata":{"id":"v4jq5W6J6n4L"},"source":["User profile data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OT8uTkB16n4L"},"outputs":[],"source":["### Users\n","\n","# Genuine accounts.\n","gen = pd.read_csv('/content/genuine_accounts.csv/users.csv')\n","\n","# Social spambots.\n","ss1 = pd.read_csv('/content/social_spambots_1.csv/users.csv')\n","ss2 = pd.read_csv('/content/social_spambots_2.csv/users.csv')\n","ss3 = pd.read_csv('/content/social_spambots_3.csv/users.csv')\n","\n","# Traditional spambots.\n","ts1 = pd.read_csv('/content/traditional_spambots_1.csv/users.csv')\n","ts2 = pd.read_csv('/content/traditional_spambots_2.csv/users.csv')\n","ts3 = pd.read_csv('/content/traditional_spambots_3.csv/users.csv')\n","ts4 = pd.read_csv('/content/traditional_spambots_4.csv/users.csv')"]},{"cell_type":"markdown","metadata":{"id":"wWM7GhfY6n4M"},"source":["Tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":648,"status":"error","timestamp":1644237739214,"user":{"displayName":"Mahesh chandra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgpSxS5Pf7uvPBbQsZ2Ztm996DUzNgWredGCjaOCQ=s64","userId":"16718463301519824511"},"user_tz":-330},"id":"SsMbIWz86n4N","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3cafaf5b-f8da-4803-c173-78feb26ab5da"},"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-66d0328ff5b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Genuine accounts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgen_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/genuine_accounts.csv/tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Social spambots.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 204140: invalid continuation byte"]}],"source":["### Tweets\n","\n","# Genuine accounts.\n","gen_tweets = pd.read_csv('/content/genuine_accounts.csv/tweets.csv')\n","\n","# Social spambots.\n","ss1_tweets = pd.read_csv('/content/social_spambots_1.csv/tweets.csv')\n","ss2_tweets = pd.read_csv('/content/social_spambots_2.csv/tweets.csv')\n","ss3_tweets = pd.read_csv('/content/social_spambots_3.csv/tweets.csv')\n","\n","# Traditional spambots.\n","ts1_tweets = pd.read_csv('/content/traditional_spambots_1.csv/tweets.csv')"]},{"cell_type":"markdown","metadata":{"id":"zgfD1gYp6n4O"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"cL4liSx06n4O"},"source":["#### Digital DNA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tq4Jf1R_6n4P"},"outputs":[],"source":["def create_dna_from_tweets(tweets_df):\n","    '''For each user id in tweets_df return a digital DNA string based on posting behaviour.'''\n","    \n","    # Add columns for counts of tweets, replies and retweets.\n","    tweets_df['num_retweets'] = np.where(tweets_df['retweeted_status_id'] == 0, 0, 1)\n","    tweets_df['num_replies'] = np.where(tweets_df['in_reply_to_status_id'] == 0, 0, 1)\n","    tweets_df['num_tweets'] = np.where((tweets_df['num_retweets'] == 0) & (tweets_df['num_replies'] == 0), 1, 0)\n","\n","    # DNA alphabet for tweet (A), retweet (C) and reply (T).\n","    tweets = tweets_df['num_tweets'] == 1\n","    retweets = tweets_df['num_retweets'] == 1\n","    replies = tweets_df['num_replies'] == 1\n","\n","    tweets_df.loc[:, 'DNA'] = np.where(retweets, 'C', np.where(replies, 'T', 'A'))\n","\n","    # Sort tweets by timestamp..\n","    tweets_df = tweets_df[['user_id', 'timestamp', 'DNA']]\n","    tweets_df = tweets_df.sort_values(by=['timestamp'])\n","    tweets_df = tweets_df[['user_id', 'DNA']]\n","    #print(tweets_df)\n","    # Create digital DNA string for each user account.\n","    #print(tweets_df[tweets_df.user_id.isin(tweets_df.groupby('user_id').filter(lambda x: len(x) <= 200))])\n","    dna = tweets_df.groupby(by=['user_id'])['DNA'].agg(lambda x: ''.join(x))\n","    \n","    return dna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LBsrqeE6n4Q"},"outputs":[],"source":["def compress_dna_df(dna):\n","    '''Return a dataframe with compression facts for a series of dna.'''\n","\n","    # Convert DNA in string object to bytes object.\n","    dna_bytes = dna.apply(lambda s: s.encode('utf-8'))\n","\n","    # Run compression on each DNA string in the sample.\n","    dna_compressed = dna_bytes.apply(lambda b: zlib.compress(b))\n","\n","    # Create dataframe with compression facts.\n","    dna_df = pd.DataFrame({'dna': dna,\n","                           'original_dna_size': dna_bytes.apply(sys.getsizeof), \n","                           'compressed_dna_size': dna_compressed.apply(sys.getsizeof)})\n","    \n","    dna_df['compression_ratio'] = dna_df['original_dna_size'] / dna_df['compressed_dna_size']\n","    \n","    return dna_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nA4DAc5NBgDC"},"outputs":[],"source":["gen_tweets['num_hashtags'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzV8d-NHBwg4"},"outputs":[],"source":["gen_dna = create_dna_from_tweets(gen_tweets)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6RCw_zPdHmE"},"outputs":[],"source":["gen_dna.head()"]},{"cell_type":"markdown","metadata":{"id":"93w5SbcG6n4R"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"JzrExDZe6n4R"},"source":["Create DNA strings based on behaviour activity for each account."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ja5OfkPx6n4R"},"outputs":[],"source":["gen_dna = create_dna_from_tweets(gen_tweets)\n","\n","ss1_dna = create_dna_from_tweets(ss1_tweets)\n","ss2_dna = create_dna_from_tweets(ss2_tweets)\n","ss3_dna = create_dna_from_tweets(ss3_tweets)\n","\n","ts1_dna = create_dna_from_tweets(ts1_tweets)"]},{"cell_type":"markdown","metadata":{"id":"qum5mJLx6n4S"},"source":["Compression on digital DNA strings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmPkMChg6n4T"},"outputs":[],"source":["gen_dna = compress_dna_df(gen_dna)\n","\n","ss1_dna = compress_dna_df(ss1_dna)\n","ss2_dna = compress_dna_df(ss2_dna)\n","ss3_dna = compress_dna_df(ss3_dna)\n","\n","ts1_dna = compress_dna_df(ts1_dna)\n","\n","bot_dna = pd.concat([ss1_dna, ss2_dna, ss3_dna])"]},{"cell_type":"markdown","metadata":{"id":"PHT7Bz2M6n4T"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"vYPL_3LF6n4T"},"source":["#### Visualise compression facts for accounts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrRajUwm6n4U"},"outputs":[],"source":["# Add a column for account type to the dataframes.\n","gen_dna['Account Type'] = 'Genuine'\n","bot_dna['Account Type'] = 'Bot'\n","\n","# Combine all samples into a single dataframe.\n","all_dna = pd.concat([gen_dna, bot_dna])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kBpb5n46n4U"},"outputs":[],"source":["all_dna.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0Wcmf9j_gVE"},"outputs":[],"source":["all_dna.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWeSCkJP8X9U"},"outputs":[],"source":["def func(x):\n","  ans = ''\n","  #print(x)\n","  if pd.isnull(x) == True:\n","    return ans\n","  else:\n","    for i in range(min(200, len(x))):\n","      #print(i)\n","      ans = ans + x[i]\n","    return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"514eT1YK6SHa"},"outputs":[],"source":["\n","dna = all_dna['dna'].apply(lambda x: func(x))\n","all_dna.drop('dna', axis=1, inplace = True)\n","dna = pd.merge(left=dna, right=all_dna, how='left', on='user_id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWBAvKfcAGsv"},"outputs":[],"source":["dna['Account Type'].value_counts()"]},{"cell_type":"code","source":["dna.drop(['original_dna_size', 'compressed_dna_size', 'compression_ratio'], axis = 1, inplace = True)\n","comp_stats = compress_dna_df(dna['dna'])\n","dna = pd.merge(left = dna, right = comp_stats, how = 'left', on = 'user_id')"],"metadata":{"id":"Qvve4w0ir_pq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRs0PwzYKO1B"},"outputs":[],"source":["dna"]},{"cell_type":"markdown","metadata":{"id":"y5o3S3m06n4V"},"source":["##### Plot of Uncompressed DNA Size vs. Compressed DNA Size.\n","\n","Includes the following accounts:\n","1. Genuine users\n","1. Social Spambots \\#1\n","1. Social Spambots \\#2\n","1. Social Spambots \\#3\n","1. Traditional Spambots \\#1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNKHPi8S6n4W"},"outputs":[],"source":["sns.set_style(\"white\")\n","\n","pal = {\n","    'Genuine': 'green',\n","    'Bot': 'red'\n","}\n","\n","g = sns.FacetGrid(dna, hue='Account Type', palette=pal, size=7, hue_kws=dict(marker=[\"o\", \"x\"]))\n","g.map(plt.scatter, \"original_dna_size\", \"compressed_dna_size\", s=25, alpha=.6, edgecolor=\"white\")\n","g.add_legend()\n","g.set_axis_labels('Original DNA Size', 'Compressed DNA Size')"]},{"cell_type":"markdown","metadata":{"id":"kvpBLqgX6n4W"},"source":["##### Plot of DNA Size vs. Compression Ratio.\n","\n","Includes the following accounts:\n","1. Genuine users\n","1. Social Spambots \\#1\n","1. Social Spambots \\#2\n","1. Social Spambots \\#3\n","1. Traditional Spambots \\#1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THFXRXG_6n4W"},"outputs":[],"source":["sns.set_style(\"white\")\n","\n","pal = {\n","    'Genuine': 'green',\n","    'Bot': 'red'\n","}\n","\n","g = sns.FacetGrid(dna, hue='Account Type', palette=pal, size=7, hue_kws=dict(marker=[\"o\", \"x\"]))\n","g.map(plt.scatter, \"original_dna_size\", \"compression_ratio\", s=25, alpha=.6, edgecolor=\"white\")\n","g.add_legend()\n","g.set_axis_labels('Original DNA Size', 'Compression Ratio')"]},{"cell_type":"markdown","metadata":{"id":"wJs_SOs26n4X"},"source":["##### Historgram for Compression Ratio\n","\n","Includes the following accounts:\n","1. Genuine users\n","1. Social Spambots \\#1\n","1. Social Spambots \\#2\n","1. Social Spambots \\#3\n","1. Traditional Spambots \\#1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxL9oUib6n4X"},"outputs":[],"source":["g = sns.FacetGrid(dna, hue='Account Type', palette=pal, size=5)\n","g.set(xlim=(0, 10))\n","g.map(sns.kdeplot, 'compression_ratio')\n","g.add_legend()"]},{"cell_type":"markdown","metadata":{"id":"f7d9F8Jh6n4Y"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"0JklzyvQ6n4Y"},"source":["Evaluation Metrics\n","1. Accuracy\n","1. Precision\n","1. Recall\n","1. F1 Score\n","1. Specificity\n","1. MCC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG-Ha2pC6n4Y"},"outputs":[],"source":["def evaluate(y_test, predictions):\n","    '''Return a dataframe with accuracy, precision, recall and f1 scores for predictions.'''\n","    \n","    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n","    specificity = tn / (tn+fp)\n","    \n","    results = [\n","        {\n","            'Metric': 'Accuracy',\n","            'Score': accuracy_score(y_test, predictions)\n","        },\n","        {\n","            'Metric': 'Precision',\n","            'Score': precision_score(y_test, predictions)\n","        },\n","        {\n","            'Metric': 'Recall',\n","            'Score': recall_score(y_test, predictions)\n","        },\n","        {\n","            'Metric': 'F1 Score',\n","            'Score': f1_score(y_test, predictions)\n","        },\n","        {\n","            'Metric': 'MCC',\n","            'Score': matthews_corrcoef(y_test, predictions)\n","        },\n","        {\n","            'Metric': 'Specificity',\n","            'Score': specificity\n","        },\n","    ]\n","\n","    return pd.DataFrame(results)"]},{"cell_type":"markdown","metadata":{"id":"EfSRDZHP6n4Z"},"source":["<hr>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9qq0CHZ6n4Z"},"outputs":[],"source":["def train_predict_evaluate(dna, features, test_size=0.5):\n","    '''Train a Logistic Regression model with given features\n","    using default parameters on a random sample of data.'''\n","\n","    # Randomly shuffle the dna dataframe.\n","    dna = utils.shuffle(dna)\n","\n","    # Features and labels.\n","    X = dna[features]\n","    y = dna.loc[:, 'label']\n","\n","    # Split the dataset for training and testing using Logistic Regression.\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n","\n","    # Logistic Regression classifier with default parameters.\n","    classifier = LogisticRegression()\n","\n","    # Train the classifier.\n","    start_train = time.time()\n","    classifier.fit(X_train, y_train)\n","    end_train = time.time()\n","\n","    # Make predictions on the test set.\n","    start_test = time.time()\n","    predictions = classifier.predict(X_test)\n","    end_test = time.time()\n","\n","    # Evaluation on the test set.\n","    results = evaluate(y_test, predictions)\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"EGVwIHnA6n4a"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"yacIdl8C6n4a"},"source":["Results for k-common substring"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WDFUFrP6n4a"},"outputs":[],"source":["k_common_substring_1 = pd.DataFrame({'Supervised': [0.977, 0.977, 0.955, 0.982, 0.977, 0.981],\n","                                     'Unsupervised': [0.976, 0.977, 0.952, 0.982, 0.972, 0.981]}, \n","                                    index=['Accuracy', 'F1 Score', 'MCC', 'Precision', 'Recall', 'Specificity'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cvmk68M6n4a"},"outputs":[],"source":["k_common_substring_2 = pd.DataFrame({'Supervised': [0.970, 0.970, 0.940, 0.978, 0.961, 0.979], \n","                                     'Unsupervised': [0.929, 0.923, 0.867, 1.000, 0.858, 1.000]},\n","                                    index=['Accuracy', 'F1 Score', 'MCC', 'Precision', 'Recall', 'Specificity'])"]},{"cell_type":"markdown","metadata":{"id":"IRwKAOWE6n4b"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"JfBD5bnj6n4b"},"source":["## Test Set #1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFVgQy-W6n4b"},"outputs":[],"source":["# Genuine accounts in test set 1.\n","gen_test1 = gen[gen['test_set_1'] == 1]\n","\n","# Social spambots in test set 1.\n","ss1_test_1 = ss1[ss1['test_set_1'] == 1]\n","\n","\n","# Tweets for genuine accounts in test set 1.\n","gen_tweets_test1 = gen_tweets[gen_tweets['user_id'].isin(gen_test1['id'])]\n","\n","# Tweets for spambot accounts in test set 1.\n","ss1_tweets_test1 = ss1_tweets[ss1_tweets['user_id'].isin(ss1_test_1['id'])]\n","\n","\n","# DNA for genuine accounts in test set 1.\n","gen_dna_test1 = create_dna_from_tweets(gen_tweets_test1)\n","\n","# DNA for spambots in test set 1.\n","ss1_dna_test1 = create_dna_from_tweets(ss1_tweets_test1)\n","\n","\n","# DNA string compression for genuine accounts in test set 1.\n","gen_dna_test1 = compress_dna_df(gen_dna_test1)\n","\n","# DNA string compression for spambots in test set 1.\n","ss1_dna_test1 = compress_dna_df(ss1_dna_test1)\n","\n","\n","# Add a column for label to the dataframes.\n","gen_dna_test1['label'] = 0\n","ss1_dna_test1['label'] = 1"]},{"cell_type":"markdown","metadata":{"id":"HrOZRKJ_6n4c"},"source":["There are some accounts for which there are no tweets, set an empty string as the DNA sequence for such accounts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9clAKiVj6n4c"},"outputs":[],"source":["sys.getsizeof(''.encode('utf-8'))   # size in bytes of empty string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTqkQytL6n4c"},"outputs":[],"source":["sys.getsizeof(zlib.compress(''.encode('utf-8')))    # size in bytes of compressed empty string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FC7yWj9m6n4d"},"outputs":[],"source":["# compression ratio for empty string\n","sys.getsizeof(''.encode('utf-8')) / sys.getsizeof(zlib.compress(''.encode('utf-8')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwJlpKJM6n4d"},"outputs":[],"source":["gen_with_no_tweets_test1 = gen_test1[~gen_test1['id'].isin(gen_dna_test1.index)]\n","\n","gen_with_no_tweets_dna_test1 = pd.DataFrame({'id': gen_with_no_tweets_test1['id'], \n","                                             'original_dna_size': 33, \n","                                             'compressed_dna_size': 41, \n","                                             'compression_ratio': 0.80, \n","                                             'label': 0})\n","\n","gen_with_no_tweets_dna_test1 = gen_with_no_tweets_dna_test1.set_index('id')\n","\n","gen_dna_test1 = pd.concat([gen_dna_test1, gen_with_no_tweets_dna_test1])\n","\n","# Combine test set 1 accounts into a single dataframe.\n","dna_test1 = pd.concat([gen_dna_test1, ss1_dna_test1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wp7qxS72ynRn"},"outputs":[],"source":["gen_dna_test1['label'].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3ItDEfiyyGt"},"outputs":[],"source":["ss1_dna_test1.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46I4jkxD6n4e"},"outputs":[],"source":["dna_test1.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hm-lVpBMFgAs"},"outputs":[],"source":["dna = dna_test1['dna'].apply(lambda x: func(x))\n","dna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgOHh6GiBETH"},"outputs":[],"source":["\n","dna_test1.drop('dna', axis=1, inplace = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlBtNM-7GQ25"},"outputs":[],"source":["dna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlytBmvmGS3-"},"outputs":[],"source":["dna_test1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMn_GqpjGPb-"},"outputs":[],"source":["dna = pd.merge(left=dna, right=dna_test1, left_index=True, right_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN8bfzMjBW9B"},"outputs":[],"source":["dna.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHSoxxnwGkdM"},"outputs":[],"source":["dna['label'].value_counts()"]},{"cell_type":"code","source":["dna.drop(['original_dna_size', 'compressed_dna_size', 'compression_ratio'], axis = 1, inplace = True)\n","comp_stats = compress_dna_df(dna['dna'])\n"],"metadata":{"id":"J703MXgBuBXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dna = pd.merge(left = dna, right = comp_stats, how = 'left', left_index = True, right_index = True)"],"metadata":{"id":"2JaMCXtk5JBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dna"],"metadata":{"id":"LBgYleVZ5x6o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZ2MzAaM6n4e"},"source":["1. Logistic Regression with Original DNA Size + Compressed DNA Size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A48IyLkF6n4f"},"outputs":[],"source":["results = pd.DataFrame()\n","\n","for i in range(1000):\n","    np.random.seed(i)\n","    results = pd.concat([results, train_predict_evaluate(dna, ['original_dna_size', 'compressed_dna_size'])])\n","\n","# Compute average scores for each metric\n","\n","results_with_length = results.groupby(by=['Metric']) \\\n","               .mean() \\\n","               .rename({'Score': 'Mean'}, axis=1) \\\n","               .join(results.groupby(by=['Metric'])\n","                            .std() \\\n","                            .rename({'Score': 'Std.'}, axis=1))\n","\n","# Compute statistical significance against the method k-common-substring for DNA sequences\n","stat_sig = []\n","for metric in k_common_substring_1.index:\n","    s = dict()\n","\n","    ts, ps = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_1['Supervised'][metric])\n","    tu, pu = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_1['Unsupervised'][metric])\n","\n","    s['Metric'] = metric\n","    s['t-supervised'] = round(ts, 4)\n","    s['t-unsupervised'] = round(tu, 4)\n","    s['p-supervised'] = round(ps, 4)\n","    s['p-unsupervised'] = round(pu, 4)\n","    s['Mean'] = results[results['Metric'] == metric]['Score'].mean()\n","    s['SD'] = results[results['Metric'] == metric]['Score'].std()\n","    s['k-supervised'] = k_common_substring_1['Supervised'][metric]\n","    s['k-unsupervised'] = k_common_substring_1['Unsupervised'][metric]\n","    \n","    stat_sig.append(s)\n","\n","stat_sig = pd.DataFrame(stat_sig).set_index(['Metric'])\n","\n","stat_sig"]},{"cell_type":"markdown","metadata":{"id":"ac0R3xpA6n4f"},"source":["2. Logistic Regression with Original DNA Size + Compression Ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUK7E_k86n4g"},"outputs":[],"source":["results = pd.DataFrame()\n","for i in range(1000):\n","    np.random.seed(i)\n","    results = pd.concat([results, train_predict_evaluate(dna, ['original_dna_size', 'compression_ratio'])])\n","\n","# Compute average scores for each metric\n","results_with_ratio = results.groupby(by=['Metric']) \\\n","               .mean() \\\n","               .rename({'Score': 'Mean'}, axis=1) \\\n","               .join(results.groupby(by=['Metric'])\n","                            .std() \\\n","                            .rename({'Score': 'Std.'}, axis=1))\n","\n","# Compute statistical significance against the method k-common-substring for DNA sequences\n","stat_sig = []\n","for metric in k_common_substring_1.index:\n","    s = dict()\n","\n","    ts, ps = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_1['Supervised'][metric])\n","    tu, pu = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_1['Unsupervised'][metric])\n","\n","    s['Metric'] = metric\n","    s['t-supervised'] = round(ts, 4)\n","    s['t-unsupervised'] = round(tu, 4)\n","    s['p-supervised'] = round(ps, 4)\n","    s['p-unsupervised'] = round(pu, 4)\n","    s['Mean'] = results[results['Metric'] == metric]['Score'].mean()\n","    s['SD'] = results[results['Metric'] == metric]['Score'].std()\n","    s['k-supervised'] = k_common_substring_1['Supervised'][metric]\n","    s['k-unsupervised'] = k_common_substring_1['Unsupervised'][metric]\n","    \n","    stat_sig.append(s)\n","\n","stat_sig = pd.DataFrame(stat_sig).set_index(['Metric'])\n","\n","stat_sig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W1Bz-OK6n4g"},"outputs":[],"source":["print('String Compression - Compressed DNA Size &',\n","      '{:.3f} &'.format(results_with_length['Mean']['Accuracy']),\n","      '{:.3f} &'.format(results_with_length['Mean']['Precision']),\n","      '{:.3f} &'.format(results_with_length['Mean']['Recall']),\n","      '{:.3f} &'.format(results_with_length['Mean']['F1 Score']),\n","      '{:.3f} &'.format(results_with_length['Mean']['MCC']),\n","      '{:.3f} \\\\\\\\'.format(results_with_length['Mean']['Specificity']))\n","\n","print('String Compression - Compression Ratio &',\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Accuracy']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Precision']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Recall']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['F1 Score']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['MCC']),\n","      '{:.3f} \\\\\\\\'.format(results_with_ratio['Mean']['Specificity']))"]},{"cell_type":"markdown","metadata":{"id":"7amAF3Fl6n4h"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"68Z0yfrR6n4h"},"source":["## Test Set #2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNKyuJv06n4h"},"outputs":[],"source":["# Genuine accounts in test set 2.\n","gen_test2 = gen[gen['test_set_2'] == 1]\n","\n","# Social spambots in test set 2.\n","ss3_test_2 = ss3[ss3['test_set_2'] == 1]\n","\n","\n","# Tweets for genuine accounts in test set 2.\n","gen_tweets_test2 = gen_tweets[gen_tweets['user_id'].isin(gen_test2['id'])]\n","\n","# Tweets for spambot accounts in test set 2.\n","ss3_tweets_test2 = ss3_tweets[ss3_tweets['user_id'].isin(ss3_test_2['id'])]\n","\n","\n","# DNA for genuine accounts in test set 2.\n","gen_dna_test2 = create_dna_from_tweets(gen_tweets_test2)\n","\n","# DNA for spambots in test set 2.\n","ss3_dna_test2 = create_dna_from_tweets(ss3_tweets_test2)\n","\n","\n","# DNA string compression for genuine accounts in test set 2.\n","gen_dna_test2 = compress_dna_df(gen_dna_test2)\n","\n","# DNA string compression for spambots in test set 2.\n","ss3_dna_test2 = compress_dna_df(ss3_dna_test2)\n","\n","\n","# Add a column for label to the dataframes.\n","gen_dna_test2['label'] = 0\n","ss3_dna_test2['label'] = 1\n","\n","\n","# Combine test set 2 accounts into a single dataframe.\n","dna_test2 = pd.concat([gen_dna_test2, ss3_dna_test2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfovs3O56n4i"},"outputs":[],"source":["gen_with_no_tweets_test2 = gen_test2[~gen_test2['id'].isin(gen_dna_test2.index)]\n","\n","gen_with_no_tweets_dna_test2 = pd.DataFrame({'id': gen_with_no_tweets_test2['id'], \n","                                             'original_dna_size': 33, \n","                                             'compressed_dna_size': 41, \n","                                             'compression_ratio': 0.80, \n","                                             'label': 0})\n","\n","gen_with_no_tweets_dna_test2 = gen_with_no_tweets_dna_test2.set_index('id')\n","\n","gen_dna_test2 = pd.concat([gen_dna_test2, gen_with_no_tweets_dna_test2])\n","\n","# Combine test set 1 accounts into a single dataframe.\n","dna_test2 = pd.concat([gen_dna_test2, ss3_dna_test2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sU5D43boISY2"},"outputs":[],"source":["dna = dna_test2['dna'].apply(lambda x: func(x))\n","dna_test2.drop('dna', axis = 1, inplace = True)\n","dna = pd.merge(dna, dna_test2, left_index=True, right_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8WhCHO-IyYE"},"outputs":[],"source":["dna"]},{"cell_type":"code","source":["dna.drop(['original_dna_size', 'compressed_dna_size', 'compression_ratio'], axis = 1, inplace = True)\n","comp_stats = compress_dna_df(dna['dna'])\n","dna = pd.merge(left = dna, right = comp_stats, how = 'left', on = 'user_id')"],"metadata":{"id":"iGyzyjJCuLrC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3o7reJY6n4i"},"source":["1. Logistic Regression with Original DNA Size + Compressed DNA Size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMjx8j1M6n4i"},"outputs":[],"source":["results = pd.DataFrame()\n","\n","for i in range(1000):\n","    np.random.seed(i)\n","    results = pd.concat([results, train_predict_evaluate(dna, ['original_dna_size', 'compressed_dna_size'])])\n","\n","results_with_length = results.groupby(by=['Metric']) \\\n","                           .mean() \\\n","                           .rename({'Score': 'Mean'}, axis=1) \\\n","                           .join(results.groupby(by=['Metric'])\n","                                        .std() \\\n","                                        .rename({'Score': 'Std.'}, axis=1))\n","# Compute average scores for each metric\n","results_with_length = results.groupby(by=['Metric']) \\\n","               .mean() \\\n","               .rename({'Score': 'Mean'}, axis=1) \\\n","               .join(results.groupby(by=['Metric'])\n","                            .std() \\\n","                            .rename({'Score': 'Std.'}, axis=1))\n","\n","# Compute statistical significance against the method k-common-substring for DNA sequences\n","stat_sig = []\n","for metric in k_common_substring_2.index:\n","    s = dict()\n","\n","    ts, ps = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_2['Supervised'][metric])\n","    tu, pu = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_2['Unsupervised'][metric])\n","\n","    s['Metric'] = metric\n","    s['t-supervised'] = round(ts, 4)\n","    s['t-unsupervised'] = round(tu, 4)\n","    s['p-supervised'] = round(ps, 4)\n","    s['p-unsupervised'] = round(pu, 4)\n","    s['Mean'] = results[results['Metric'] == metric]['Score'].mean()\n","    s['SD'] = results[results['Metric'] == metric]['Score'].std()\n","    s['k-supervised'] = k_common_substring_2['Supervised'][metric]\n","    s['k-unsupervised'] = k_common_substring_2['Unsupervised'][metric]\n","    \n","    stat_sig.append(s)\n","\n","stat_sig = pd.DataFrame(stat_sig).set_index(['Metric'])\n","\n","stat_sig"]},{"cell_type":"markdown","metadata":{"id":"bLZZnxZw6n4j"},"source":["2. Logistic Regression with Original DNA Size + Compression Ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QBmE4g66n4j"},"outputs":[],"source":["results = pd.DataFrame()\n","for i in range(1000):\n","    np.random.seed(i)\n","    results = pd.concat([results, train_predict_evaluate(dna, ['original_dna_size', 'compression_ratio'])])\n","\n","# Compute average scores for each metric\n","results_with_ratio = results.groupby(by=['Metric']) \\\n","               .mean() \\\n","               .rename({'Score': 'Mean'}, axis=1) \\\n","               .join(results.groupby(by=['Metric'])\n","                            .std() \\\n","                            .rename({'Score': 'Std.'}, axis=1))\n","\n","# Compute statistical significance against the method k-common-substring for DNA sequences\n","stat_sig = []\n","for metric in k_common_substring_2.index:\n","    s = dict()\n","\n","    ts, ps = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_2['Supervised'][metric])\n","    tu, pu = ttest_1samp(results[results['Metric'] == metric]['Score'], \n","                         k_common_substring_2['Unsupervised'][metric])\n","\n","    s['Metric'] = metric\n","    s['t-supervised'] = round(ts, 4)\n","    s['t-unsupervised'] = round(tu, 4)\n","    s['p-supervised'] = round(ps, 4)\n","    s['p-unsupervised'] = round(pu, 4)\n","    s['Mean'] = results[results['Metric'] == metric]['Score'].mean()\n","    s['SD'] = results[results['Metric'] == metric]['Score'].std()\n","    s['k-supervised'] = k_common_substring_2['Supervised'][metric]\n","    s['k-unsupervised'] = k_common_substring_2['Unsupervised'][metric]\n","    \n","    stat_sig.append(s)\n","\n","stat_sig = pd.DataFrame(stat_sig).set_index(['Metric'])\n","\n","stat_sig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jII4SFyM6n4k"},"outputs":[],"source":["print('String Compression - Compressed DNA Size &',\n","      '{:.3f} &'.format(results_with_length['Mean']['Accuracy']),\n","      '{:.3f} &'.format(results_with_length['Mean']['Precision']),\n","      '{:.3f} &'.format(results_with_length['Mean']['Recall']),\n","      '{:.3f} &'.format(results_with_length['Mean']['F1 Score']),\n","      '{:.3f} &'.format(results_with_length['Mean']['MCC']),\n","      '{:.3f} \\\\\\\\'.format(results_with_length['Mean']['Specificity']))\n","\n","print('String Compression - Compression Ratio &',\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Accuracy']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Precision']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['Recall']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['F1 Score']),\n","      '{:.3f} &'.format(results_with_ratio['Mean']['MCC']),\n","      '{:.3f} \\\\\\\\'.format(results_with_ratio['Mean']['Specificity']))"]},{"cell_type":"markdown","metadata":{"id":"XtEHjPHq6n4k"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"PL0Afk2f6n4k"},"source":["#### Compression Statistics over the test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qz3_Vg806n4k"},"outputs":[],"source":["# Filter genuine accounts and bot accounts to compute statistics for each group.\n","g = all_dna['Account Type'] == 'Genuine'\n","b = all_dna['Account Type'] == 'Bot'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VFBFyf26n4l"},"outputs":[],"source":["compression_stats = pd.DataFrame({'Mean Length': [all_dna['dna'].apply(len).mean(), \n","                                                  all_dna[g]['dna'].apply(len).mean(), \n","                                                  all_dna[b]['dna'].apply(len).mean()],\n","                                  'SD Length': [all_dna['dna'].apply(len).std(), \n","                                                all_dna[g]['dna'].apply(len).std(), \n","                                                all_dna[b]['dna'].apply(len).std()],\n","                                  'Mean Uncompressed Size': [all_dna['original_dna_size'].mean(), \n","                                                             all_dna[g]['original_dna_size'].mean(), \n","                                                             all_dna[b]['original_dna_size'].mean()],\n","                                  'SD Uncompressed Size': [all_dna['original_dna_size'].std(), \n","                                                            all_dna[g]['original_dna_size'].std(), \n","                                                           all_dna[b]['original_dna_size'].std()],\n","                                  'Mean Compressed Size': [all_dna['compressed_dna_size'].mean(), \n","                                                           all_dna[g]['compressed_dna_size'].mean(), \n","                                                           all_dna[b]['compressed_dna_size'].mean()],\n","                                  'SD Compressed Size': [all_dna['compressed_dna_size'].std(), \n","                                                         all_dna[g]['compressed_dna_size'].std(), \n","                                                         all_dna[b]['compressed_dna_size'].std()],\n","                                  'Mean Compressio Ratio': [all_dna['compression_ratio'].mean(), \n","                                                            all_dna[g]['compression_ratio'].mean(), \n","                                                            all_dna[b]['compression_ratio'].mean()],\n","                                  'SD Compressio Ratio': [all_dna['compression_ratio'].std(), \n","                                                          all_dna[g]['compression_ratio'].std(), \n","                                                          all_dna[b]['compression_ratio'].std()]},\n","                                 index=['All', 'Genuine', 'Bot'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUU3ok7t6n4l"},"outputs":[],"source":["compression_stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Im4lcz_26n4l"},"outputs":[],"source":["print('All &',\n","      '{:.2f} & '.format(compression_stats['Mean Length']['All']),\n","      '{:.2f} & '.format(compression_stats['SD Length']['All']),\n","      '{:.2f} & '.format(compression_stats['Mean Uncompressed Size']['All']),\n","      '{:.2f} & '.format(compression_stats['SD Uncompressed Size']['All']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressed Size']['All']),\n","      '{:.2f} & '.format(compression_stats['SD Compressed Size']['All']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressio Ratio']['All']),\n","      '{:.2f} \\\\\\\\ '.format(compression_stats['SD Compressio Ratio']['All']),\n","     )\n","\n","print('Genuine &',\n","      '{:.2f} & '.format(compression_stats['Mean Length']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['SD Length']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['Mean Uncompressed Size']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['SD Uncompressed Size']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressed Size']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['SD Compressed Size']['Genuine']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressio Ratio']['Genuine']),\n","      '{:.2f} \\\\\\\\ '.format(compression_stats['SD Compressio Ratio']['Genuine']),\n","     )\n","\n","print('Bot &',\n","      '{:.2f} & '.format(compression_stats['Mean Length']['Bot']),\n","      '{:.2f} & '.format(compression_stats['SD Length']['Bot']),\n","      '{:.2f} & '.format(compression_stats['Mean Uncompressed Size']['Bot']),\n","      '{:.2f} & '.format(compression_stats['SD Uncompressed Size']['Bot']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressed Size']['Bot']),\n","      '{:.2f} & '.format(compression_stats['SD Compressed Size']['Bot']),\n","      '{:.2f} & '.format(compression_stats['Mean Compressio Ratio']['Bot']),\n","      '{:.2f} \\\\\\\\ '.format(compression_stats['SD Compressio Ratio']['Bot']),\n","     )"]},{"cell_type":"markdown","metadata":{"id":"A-aQybeq6n4m"},"source":["#### Histogram of compression ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpcohGYG6n4m"},"outputs":[],"source":["g = sns.FacetGrid(all_dna, hue='Account Type', palette=pal, height=5)\n","g.set(xlim=(0, 10))\n","g.map(sns.kdeplot, 'compression_ratio')\n","g.add_legend()"]},{"cell_type":"markdown","metadata":{"id":"GH-MhFMm6n4n"},"source":["#### Boxplot of compression ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYh_IgHS6n4n"},"outputs":[],"source":["sns.boxplot(data=all_dna, x='Account Type', y='compression_ratio', palette=pal, orient='v')"]},{"cell_type":"markdown","metadata":{"id":"aDjbGelc6n4n"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"8ywPw7Ys6n4n"},"source":["##### Plot of Uncompressed DNA Size vs. Compressed DNA Size.\n","\n","Includes the following accounts:\n","1. Genuine users\n","1. Social Spambots \\#1\n","1. Social Spambots \\#3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvbAm-sJ6n4o"},"outputs":[],"source":["dna_all = pd.concat([dna_test1, dna_test2]).rename({'label': 'Account Type'}, axis=1)\n","dna_all['Account Type'] = np.where(dna_all['Account Type'] == 0, 'Genuine User', 'Bot Account')\n","\n","sns.set(style='white', font='Source Sans Pro')\n","\n","pal = {\n","    'Genuine User': sns.color_palette('Dark2')[0],\n","    'Bot Account':  sns.color_palette('Dark2')[1]\n","}\n","\n","g = sns.FacetGrid(dna_all, hue='Account Type', \n","                  palette=pal, height=6, hue_kws=dict(marker=[\"^\", \".\"]))\n","g.map(plt.scatter, \"original_dna_size\", \"compressed_dna_size\", s=100, alpha=0.85, edgecolor=\"white\")\n","# g.add_legend()\n","plt.legend(loc='upper left', frameon=False)\n","g.set_axis_labels('Original DNA Size', 'Compressed DNA Size')\n","g.savefig('dna-scatter-1.pdf')"]},{"cell_type":"markdown","metadata":{"id":"PHYrt-fT6n4o"},"source":["##### Plot of DNA Size vs. Compression Ratio.\n","\n","Includes the following accounts:\n","1. Genuine users\n","1. Social Spambots \\#1\n","1. Social Spambots \\#3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCGnG1R26n4o"},"outputs":[],"source":["dna_all = pd.concat([dna_test1, dna_test2]).rename({'label': 'Account Type'}, axis=1)\n","dna_all['Account Type'] = np.where(dna_all['Account Type'] == 0, 'Genuine User', 'Bot Account')\n","\n","sns.set(style='white', font='Source Sans Pro')\n","\n","pal = {\n","    'Genuine User': sns.color_palette('Dark2')[0],\n","    'Bot Account':  sns.color_palette('Dark2')[1]\n","}\n","\n","g = sns.FacetGrid(dna_all, hue='Account Type', \n","                  palette=pal, height=6, hue_kws=dict(marker=[\"^\", \".\"]))\n","g.map(plt.scatter, \"original_dna_size\", \"compression_ratio\", s=100, alpha=0.85, edgecolor=\"white\")\n","# g.add_legend()\n","plt.legend(loc='upper left', frameon=False)\n","g.set_axis_labels('Original DNA Size', 'Compression Ratio')\n","g.savefig('dna-scatter-2.pdf')"]},{"cell_type":"markdown","metadata":{"id":"e5v59TFD6n4p"},"source":["<hr>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWYDbmLJ6n4p"},"outputs":[],"source":["# markers = {'Genuine': '^', 'Bot': '+'}\n","\n","# ax = sns.scatterplot(data=dna_all, x='original_dna_size', y='compressed_dna_size', s=50,\n","#                 hue='Account Type', palette=pal, style='Account Type', alpha=0.7)\n","\n","# ax.set_xlabel('Original DNA Size')\n","# ax.set_ylabel('Compressed DNA Size')"]}],"metadata":{"colab":{"name":"Cresci17_dna-compression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}